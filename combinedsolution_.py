# -*- coding: utf-8 -*-
"""combinedSolution .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d1SHmb0Gnf5LrQRyEYXrVA8N9TU6FIAb

#Global Vars
"""
import tensorflow as tf
import numpy as np
from scipy import special as sp
import math
tf.config.run_functions_eagerly(True)
print(tf.__version__)

#channel data
size_area=200.0 # size of area sensors are distributed 
pu_active_prob = 0.7
pl_const = 34.5 # use pathloss constant   (wslee paper)
pl_alpha = 38.0 # use pathloss constant   (wslee paper)
d_ref = 10.0 # reference distance 
sh_sigma = 7.9 # shadow fading constant   (wslee paper)
p_t_dB = 10.0 # tx power - 23dBm
p_t = 10*(p_t_dB/10)
sigma_v = 1 # noise variance

#DNN data
alpha = 0.0001
batch_s = 512
batch_id = batch_s
num_sens = 10 
samples_factor = 100
num_samples = batch_s*samples_factor

#formula data
n=50
pi_0 = 0.3
pi_1 = 0.7
fs = 100*1000
tr = 0.2*pow(10,-3)
T_cte = (n/fs)+num_sens*tr
pf_threshold = 0.01
pd_threshold = 0.5
snrs_count = -1
counter = 1
counter_pe = 0
Pe_values = []
final_Pe_values = []
pf_values = []
pd_values = []

"""# Channel Model"""

import numpy as np
 
 
class ChannelModel:
    
    
    @staticmethod
    def get_distances() -> np.ndarray:
    
        '''
        this function generate the random distribution of secondary users and the primary user
        
        '''    
        sen_loc = size_area*(np.random.rand(num_sens, 2)-0.5)
        pri_loc = size_area*(np.random.rand(1, 2)-0.5) #placing sensing entities and primary user randomly
        dist_pr_su_vec = pri_loc.reshape(1, 2) - sen_loc # Generate PU-SU distance_vector
        dist_pr_su_vec = np.maximum(dist_pr_su_vec, 0.1)
        dist_pr_su_vec = np.linalg.norm(dist_pr_su_vec, axis=1)
        dist_su_su_vec = sen_loc.reshape(num_sens, 1, 2) - sen_loc # Generate SU-SU distance_vector
        dist_su_su_vec = np.linalg.norm(dist_su_su_vec, axis=2)
        
        return dist_pr_su_vec, dist_su_su_vec
    
    
    @staticmethod
    def get_channel_gain(dist_pr_su_vec: np.ndarray) -> np.ndarray:
    
        '''
        this function generates the channel gain of each secondary user using the distance
        
        Parameters:
         dist_pr_su_vec : distance between secondary users and the primary user
    
        Output:
            channel gain
        
        '''  
        
        pu_ch_gain_db = - pl_const - pl_alpha * np.log10(dist_pr_su_vec) # primary channel gain
        return 10 ** (pu_ch_gain_db / 10)
    
    
    @staticmethod
    def get_secondary_correlation(dist_su_su_vec: np.ndarray ) -> np.ndarray:
        '''
        this function computes the secondary users correlation using SU-SU distances
        
        Parameters:
         dist_su_su_vec : distances between the secondary users
    
        Output:
            secondary users correlation
        '''    
        return np.exp(-dist_su_su_vec / d_ref)
 
 
    @staticmethod
    def get_shadowing(su_cor: np.ndarray, num_sens: int) -> np.ndarray:
            '''
            this function computes the shadowing using SU-SU correlation 
            
            Parameters:
             num_sens : number of sensing units
             su_cor : the correlation between the secondary users
        
            Output:
                shadowing    
            
            ''' 
            shadowing_dB = sh_sigma * np.random.multivariate_normal(np.zeros([num_sens]), su_cor)
            return 10 ** (shadowing_dB / 10)
 
 
    @staticmethod
    def get_multiPath_Fading(num_sens: int) -> np.ndarray:
        '''
        this function computes the multipath fading 
        
        Parameters:
         num_sens : number of sensing units
    
        Output:
            multipath fading 
        '''     
        multi_fading = 0.5 * np.random.randn(num_sens) ** 2 + 0.5 * np.random.randn(num_sens) ** 2
        return multi_fading ** 0.5
 
    
    @classmethod
    def ch_gen(cls, num_samples: int) -> np.ndarray:
      """ 
      This function deploy the channel model.
       Parameters:
            num_samples : number of samples
    
        Output:
           signal noise ratio
      
      """
    
      returned_power = []
      returned_SNRs = []
      returned_gain = []
      
      
      for i in range(num_samples):
    
        dist_pr_su_vec, dist_su_su_vec = cls.get_distances()
        pu_ch_gain =  cls.get_channel_gain(dist_pr_su_vec)
        su_cor =  cls.get_secondary_correlation(dist_su_su_vec)
        shadowing =  cls.get_shadowing(su_cor,num_sens)
        pu_power = np.zeros([len(su_cor)]) #pu_power (received power initialization)
        noise = np.random.randn(num_sens)
        #SNR = np.zeros([len(su_cor)])
        pri_power = p_t #pri_power (transmitted power)
        pu_ch_gain_tot = pu_ch_gain 
        # test the activity of the primary user 
        if (np.random.rand() < pu_active_prob):
          pu_ch_gain_tot = pu_ch_gain  * shadowing + noise
          pu_power = pu_power +  pri_power*pu_ch_gain_tot 
          SNR = pri_power * pow(abs(pu_ch_gain_tot),2)/ sigma_v
        else:
          SNR = noise
        multi_fading =  cls.get_multiPath_Fading(num_sens)
        pu_power = pu_power * multi_fading
        returned_power.append(pu_power)
        returned_SNRs.append(SNR)
        returned_gain.append(pu_ch_gain_tot)
        
        
      output = dict()
      output['snrs'] = returned_SNRs
      output['gain'] = returned_gain
      
    
      return output

"""# Global DNN Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization
 
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras import regularizers
from statistics import mean 
import matplotlib.pyplot as plt
 
class DNNModel:
    

    def get_model():
        
        """ 
        This function returns sequential model and the model initilizer
      
        """
        model = Sequential()
        model.add(BatchNormalization(input_shape=(num_sens,)))
        initializer = tf.keras.initializers.GlorotUniform()

        return model, initializer     
 
 

    def train_model(model, X_train: np.ndarray, X_val: np.ndarray,
              y_train: np.ndarray,
              y_val: np.ndarray, loss_fn) -> np.ndarray:
        """
        This function trains the model. The number of epochs and 
        batch_size are set by the constants in the parameters section.
        
        Parameters:
            model : model with the chosen architecture
            X_train : training features
            y_train : training target
            X_valid : validation features
            Y_valid : validation target
            
        Output:
            model training history    
        
        """
        history = []
        opt = tf.keras.optimizers.Adam(learning_rate=1e-4)
        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50) 
        model.compile(loss = loss_fn , optimizer=opt, metrics=['acc'])
        history = model.fit(X_train,y_train,batch_size=batch_s,epochs=40,
                            validation_data=(X_val,y_val),
                            callbacks=[callback])

        return history
    
  

    def eval_metric(model, history: np.ndarray,
                    metric_name: str,problem_id: str) -> np.ndarray:
        '''
        Function to evaluate a trained model on a chosen metric. 
        Training and validation metric are plotted in a
        line chart for each epoch.
        
        Parameters:
            history : model training history
            metric_name : loss or accuracy
        Output:
            line chart with epochs of x-axis and metric on
            y-axis
        '''   
        for key in history.history.keys():
            print(key)
        metric = history.history[metric_name]
        val_metric = history.history['val_' + metric_name] 
        plt.figure(2)
        ax = plt.axes()
        ax.plot(metric, linestyle= 'dashdot', marker='+', color='blue',label='Train Loss')
        ax.plot(val_metric, linestyle='dashdot', marker='+', color='red',label='Val Loss')
        ax.set_title('Loss ' + model.name)
        ax.margins(x=0,y=0)
        ax.grid(False)
        plt.legend()
        ax.set_xlabel('Epoch number')
        ax.set_ylabel(metric_name)
        #plt.show()  
        plt.savefig(metric_name+' metric in '+model.name +' for '+problem_id +'.pdf')
        plt.close()
 
  
    def compare_models(model_1, model_2, 
                       
                       history_1: np.ndarray,
                       history_2: np.ndarray, 
                        
                      
                       metric: str, problem_id: str):
    
        '''
        Function to compare a metric between two models 
        
        Parameters:
            history_1 : training history of model 1
            history_2 : training history of model 2
            metric : metric to compare, loss, acc, val_loss or val_acc
            
        Output:
            plot of metrics of both models
        '''    
        metric_1 = history_1.history[metric]
        metric_2 = history_2.history[metric]        
        metrics_dict = {
            'acc' : 'Training Accuracy',
            'loss' : 'Training Loss',
            'val_acc' : 'Validation accuracy',
            'val_loss' : 'Validation loss'
        } 
        
        metric_label = metrics_dict[metric]
    
        plt.figure(3)
        ax = plt.axes()    
        ax.plot(metric_1, linestyle= 'solid', color='orange', label=model_1.name)
        ax.plot(metric_2, linestyle= 'solid', color='green', label=model_2.name)
        ax.margins(x=0,y=0)
        plt.xlabel('Epoch number')
        plt.ylabel(metric_label)
        plt.title('Comparing ' + metric_label + ' between models for '+ problem_id)
        plt.legend()
        plt.savefig('Comparing ' + metric_label + ' between models'+ ' for the problem '+ problem_id +'.pdf')
        plt.close()

"""# DNN Model 3 Vars"""

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras import backend as K
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras import regularizers
from numpy import exp
     
class BivariateDNNModel(DNNModel):
    @classmethod
    def bivariate_loss_gain(cls,gain):
        """
        

        Parameters
        ----------

        gain : np.ndarray
            the channel gain

        Returns
        -------
        
           loss value.

        """
        global batch_id
        global snrs_count
        global Pe_values
        global final_Pe_values
        global pf_values
        global pd_values
        
        dim=batch_s * num_sens 
        gain= gain[0:batch_id,:]
        gain= tf.cast(gain, tf.float32)
        gain_bis = tf.reshape(gain,(batch_s,num_sens))
        snrs_count = snrs_count + 1
        
        
        def bivariate_loss(snrs: np.ndarray, predicted) -> float:
          global counter
          global counter_pe
                    
          # Threshold Predicted value 
          predicted_threshold = predicted[:,0]
          
          # weights Predicted values 
          predicted_weights = predicted[:,1:]
          count = 0
          
          loss_values = []
          pe_values = []
          while count < batch_s:              
              snrs_s = snrs[count,:]
              C = tf.linalg.tensor_diag(1+2*snrs_s)
              val = predicted_threshold[count]
              diag_exp1 = tf.linalg.diag_part(tf.linalg.matmul(predicted_weights,tf.ones([num_sens,batch_s])))
              test = fs*(T_cte - num_sens* tr)*diag_exp1
              #tf.print("test",test)
              exp1 = predicted_threshold[count]-test
              #tf.print("exp1",exp1)
              diag_exp2=  tf.linalg.diag_part(tf.linalg.matmul(predicted_weights,tf.keras.backend.transpose(predicted_weights)))
              exp2 = tf.math.sqrt(2*fs*(T_cte - num_sens* tr)* diag_exp2)
             # tf.print("exp2",exp2)
              p_0 = cls.Q(exp1/exp2)
              #tf.print("po",p_0)
              diag_exp3 = tf.linalg.diag_part(tf.linalg.matmul(predicted_weights,(0.3*tf.keras.backend.transpose(gain_bis)+tf.ones([num_sens,batch_s]))))
              exp3 = predicted_threshold[count]- (fs*(T_cte - num_sens* tr))* diag_exp3
              diag_exp4 = tf.linalg.diag_part(tf.linalg.matmul(tf.linalg.matmul(predicted_weights,C),tf.keras.backend.transpose(predicted_weights)))
              exp4 = tf.math.sqrt(2*fs*(T_cte - num_sens* tr)*diag_exp4)
              p_1 = (1-cls.Q(exp3/exp4))
              Pf = cls.Q(exp1/exp2)
              Pf = tf.reduce_mean(tf.reduce_mean(Pf)) 
              #print("pf",Pf)
              if not Pf< pf_threshold:
                  Pf_penality = 0.0
              else:
                  Pf_penality = tf.math.pow((Pf-pf_threshold),2) 
              Pd = cls.Q(exp3/exp4) 
              Pd = tf.reduce_mean(tf.reduce_mean(Pd))
              if not Pd> pd_threshold:
                  Pd_penality = 0.0
              else:
                  Pd_penality = tf.math.pow((Pd-pd_threshold),2) 
              weight_penality = tf.math.pow((tf.norm(predicted_weights)-1),2) 
              Pe = pi_0*Pf+ pi_1*(1 - Pd)
              #tf.print("pe",Pe)
              loss = Pe + alpha*(Pd_penality + Pf_penality + weight_penality)
              loss = tf.reduce_mean(tf.reduce_mean(loss))# remove 
              loss_values.append(loss)
              pe_values.append(Pe.numpy())
              count = count+1
          
          if counter == 70:  
              counter = 0
              pf_values.append(Pf.numpy())
              pd_values.append(Pd.numpy())
              Pe_values.append(tf.reduce_mean(pe_values).numpy())
          counter = counter + 1 
          #tf.print("pe",tf.reduce_mean(loss_values))
          return tf.reduce_mean(loss_values)

        return bivariate_loss
  
    @staticmethod  
    def Q(x):
        """
        the Q-function
        """
        value = tf.cast(x, tf.float32)
        return 0.5-0.5*tf.math.erf(tf.cast(value/ math.sqrt(2),tf.float32))
    
    
    @staticmethod  
    def softmax(vector):
        	e = exp(vector)
        	return e / e.sum()
        
        
    @classmethod
    def choose_model(cls,choice: str):
        """ 
        This function defines the model architecture and returns the model
      
        """  
        #get_custom_objects().update({'custom_activation': Activation(cls.custom_activation)})
        model = Sequential()
        model.add(BatchNormalization(input_shape=(num_sens,)))
        initializer = tf.keras.initializers.GlorotUniform()
        if choice == "dropout":            
                model.add(Dense(num_sens+1, kernel_initializer=initializer, kernel_regularizer=regularizers.l1(0.003)))
                model.add(LeakyReLU(alpha=0.05))
                model.add(layers.Dropout(0.4))
                model.add(BatchNormalization())
                model.add(Dense(6, kernel_initializer=initializer, kernel_regularizer=regularizers.l1(0.003)))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.4))
                model.add(BatchNormalization())
                model.add(Dense(6, kernel_initializer=initializer, kernel_regularizer=regularizers.l1(0.003)))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.4))
                model.add(BatchNormalization())
                model.add(Dense(6, kernel_initializer=initializer, kernel_regularizer=regularizers.l1(0.003)))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.4))
                model.add(BatchNormalization())
                model.add(Dense(num_sens+1, name='last_layer', kernel_initializer=initializer, kernel_regularizer=regularizers.l1(0.003)))
                #model.add(Activation(cls.custom_activation, name='SpecialActivation'))
                #model.add(LeakyReLU(alpha=0.005))
                model._name = 'dropout'                
        return model
 
 
        if choice == "dropout1":            
                model.add(Dense(num_sens, kernel_initializer=initializer))
                model.add(LeakyReLU(alpha=0.05))
                model.add(layers.Dropout(0.3))
                model.add(BatchNormalization())
                model.add(Dense(64, kernel_initializer=initializer))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.3))
                model.add(BatchNormalization())
                model.add(Dense(32, kernel_initializer=initializer))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.3))
                model.add(BatchNormalization())
                model.add(Dense(16, kernel_initializer=initializer))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.3))
                model.add(BatchNormalization())
                model.add(Dense(1, name='last_layer', kernel_initializer=initializer,activation='softplus'))
                model._name = 'dropout'                
        return model
    
"""# Mathematical Model"""

import numpy as np
import math 
from scipy.optimize import minimize, rosen, rosen_der
from numpy import linalg as LA
import itertools
from numpy.linalg import multi_dot
from scipy import special
import random

class MathematicalModel:
        
    @classmethod
    def new_paper_mathematical_weights(cls,snrs: np.ndarray)-> np.ndarray:
    
        '''
        Function to compute the weights from the mathematical formula in 2016 paper
        
        Parameters:
            
            snrs:  the row of  the sensing units snrs 
            signak_power: the power of the signal received by the sensing units
            
        Output:
            mathematical weights
        '''      
        C = np.diag(1+2*snrs)
        C_inv = np.linalg.pinv(C)
        norm = np.linalg.norm(np.dot(C_inv,snrs), ord=2)  
        w_opt = np.dot( C_inv, snrs)/norm
        w_opt = cls.real(w_opt)
        w_opt = np.array(w_opt)   
        return w_opt
    
    
    @staticmethod
    def compute_deflection_coef(weights: np.ndarray, snrs: np.ndarray)-> float:
        '''
        Function to compute the deflection coef from the mathematical 
        formula in 2007 paper
        
        Parameters:
            
            snrs: the row of  the sensing units snrs 
            weights: weights array 
            
        Output:
            mathematical weights
        '''        
                
        
        snrs = snrs.transpose()
        snrs = snrs.reshape(10,1)
        weights = weights.reshape(10,1)
        t = np.dot(snrs.transpose(),weights)**2
        t1 = np.dot(4*weights.transpose(),
                    (n*np.identity(num_sens)+np.diag(snrs)))
        b = np.dot(t1,weights)       
        dm_square = t/b
        
        return dm_square
    
    
    @staticmethod
    def compute_weights_using_deflection_coef(snrs) -> np.ndarray:
        '''
        This function is the numerical computation of the weights using the minimization function
        
         Parameters:
             snrs: secondary users snrs
         output:
             the optimal weights
        
        '''
        #fun = lambda x: ((x[i]*nu[i] for i in range(0,len(nu)))**2)/(4*((n+nu[i])*x[i]**2 for i in range(0,len(nu))))
        n = 50
        nu = snrs
        fun = lambda x: - ((x[0]*nu[0] + x[1]*nu[1]+x[2]*nu[2]+x[3]*nu[3]+x[4]*nu[4]+x[5]*nu[5]+x[6]*nu[6]+x[7]*nu[7]+x[8]*nu[8]+x[9]*nu[9])**2)/(4*((n+nu[0])*x[0]**2+(n+nu[1])*x[1]**2+(n+nu[2])*x[2]**2+(n+nu[3])*x[3]**2+(n+nu[4])*x[4]**2+(n+nu[5])*x[5]**2+(n+nu[6])*x[6]**2+(n+nu[7])*x[7]**2+(n+nu[8])*x[8]**2+(n+nu[9])*x[9]**2))            
        constraint = ({'type': 'eq', 'fun': lambda x:  LA.norm(x,1)-1})
        bnds = ((0, 1),) * len(snrs)
        random_guess = np.random.dirichlet(np.ones(10)*1000.,size=1)
        random_guess = tuple(list(random_guess[0]))
        res = minimize(fun, random_guess, method='SLSQP', bounds=bnds, constraints=constraint)
        return(res.x)
    
    
    @staticmethod
    def Q(x):
        """
        the Q-function
        """
        
        return 0.5-0.5*special.erf(x/ math.sqrt(2))
    
    
    @classmethod
    def func(cls,x, *args):
    
        """
        The optimization function from the 2016 paper
        
        """
        snrs=np.array(args[0])

        epsilon = 0.05 # to avoid the division by 0 
        h = np.ones(num_sens)# I set h for ones to simplify the computation
        h = h.reshape(10,1)# I reshape the snrs for matricial multiplication 
        bold_one = np.identity(num_sens)
        C = np.diag(1+2*snrs) # the C value from the formula diag{[1 +2snrs]}
        C_inv = np.linalg.inv(C) # C_inv
        snrs_s = snrs.reshape(10,1) 
        exp1 = multi_dot([snrs_s.transpose(), C_inv.transpose(),h])
        exp2 = np.linalg.norm(np.dot( C_inv,snrs_s), ord=1)
        identity_vector = np.ones(num_sens)
        identity_vector = identity_vector.reshape(10,1)
        add = identity_vector+h       
        exp3 = num_sens* multi_dot([snrs_s.transpose(), C_inv.transpose(),add])- (x*np.linalg.norm(np.dot( C_inv,snrs_s), ord=1))
        exp4 = np.sqrt(multi_dot([snrs_s.transpose(), C_inv.transpose(), snrs_s])+epsilon)
        t1 = np.divide(exp1, exp2)
        t2 = np.divide(exp3, exp4)
        p_0 = pi_0*cls.Q(x-(exp1/exp2))
        p_1 = pi_1*cls.Q(exp3/exp4)
        f = p_0 + p_1
        print("this is the mathematical pe",f[0][0])
        return f[0][0]
    
    
    @classmethod
    def compute_numerical_thresholds(cls,snrs):
        
        '''
        This function compute the numerical optimal thresholds using minimization
        
          Parameters:
              snrs of the secondary users
          Outputs:
              numerical optimal thresholds
        '''
        bnds = ((0, 1),)  
        random_guess = random.choice(snrs)
        constraint = ({'type': 'ineq', 'fun': lambda x:  x>0})
        res = minimize(cls.func, random_guess,args=(snrs), method='SLSQP', bounds=bnds, constraints=constraint)
        return res.x 
     
    
    @staticmethod
    def real(w1):
        return w1.real
    

import math 
from scipy import special as sp
import scipy.spatial.distance
import matplotlib.pyplot as plt
from texttable import Texttable
from memory_profiler import profile
import time

class SpectrumSensing:
    
    @profile         
    def generate_thresholds(cls,snrs_test) -> np.ndarray :
        #thresholds_d = model_d.predict(snrs_test)
        #dnn_thresholds_d = thresholds_d[0]
        for i in range(0,snrs_test.shape[0]):
            numerical_thresholds = MathematicalModel.compute_numerical_thresholds(snrs_test[i])
        #print("DNN thresholds for dropeout model ",dnn_thresholds_d)
        #print("numerical thresholds",numerical_thresholds) 
        
        
"""# Main/plots"""

from sklearn import preprocessing 
from sklearn.model_selection import train_test_split 
import pickle
import os



def scaling_data(output_train):
        """
    this function scale the data 

    Parameters
    ----------
    output_train : np.ndarray
        the output of the sampling.

    Returns
    -------
    snrs_train : np.ndarray
        scaled data.

      """
    
        snrs_train = np.array(output_train['snrs'])
        scaler_train = preprocessing.StandardScaler().fit(snrs_train)
        snrs_train = scaler_train.transform(snrs_train)
        snrs_train = np.absolute(snrs_train)
        
        return snrs_train

def run_model(output_train,problem1_id):
        """
    
    this function split the data into train and val and launch the training process
    ----------
    output_train : np.ndarray
        the training data.
    problem1_id : str
        the id of the model.

    Returns
    -------
    None.

      """
        snrs_train = scaling_data(output_train)
        signal_gain= np.array(output_train['gain'])
        X_train, X_val, y_train, y_val = train_test_split(snrs_train, snrs_train, test_size=0.30)
        model = BivariateDNNModel.choose_model("dropout")
        history = BivariateDNNModel.train_model(model, X_train, X_val, y_train, y_val, BivariateDNNModel.bivariate_loss_gain(gain=signal_gain)) 
        BivariateDNNModel.eval_metric(model, history, "loss", problem1_id)

def generate_data(choice,SU_N):
     """
    this function provides two way to generate data from an existing file or a new sampling process

    Parameters
    ----------
    choice : str
        '1': from an existing file.
        '2': from a sampling process.
    SU_N : int
        number of secondary users.

    Returns
    -------
    output_train : np.ndarray
        generated samples.

    """
    
     global num_sens
     num_sens = SU_N
     file_id = str(SU_N)
     output_train =ChannelModel.ch_gen(num_samples)
     if choice == '1':
         with open('trainData'+file_id+'.data', 'rb') as filehandle:
             output_train = pickle.load(filehandle)
             
             
     if choice == '2':
         
         print("Do you want to stock this data?\n")
         print("Press y for YES and n for NO\n")
         decision = input("Press your decision: ")
         if decision == 'y':
             with open('trainData'+file_id+'.data', 'wb') as filehandle: 
                 pickle.dump(output_train, filehandle)
                 print("data is stocked")
                 pass
         else:
             pass
             
         
         
         
     return output_train
 
def plot_averagePe_Vs_SU(problem1_id):
    """
    this function plot the minimum probability of detection 
    using different number of SU configurations
    

    Parameters
    ----------
    problem1_id : str
        DNN model id.

    Returns
    -------
    None.

    """
    global num_sens
    pd = []
    x = [5,10,15,20,25,30,35,40,45,50]
    for su in x:
        num_sens = su
        file_id = str(num_sens)
        if os.path.isfile('trainData'+file_id+'.data'):
            with open('trainData'+file_id+'.data', 'rb') as filehandle:
                output_train = pickle.load(filehandle)
        else:
            output_train = generate_data('2',num_sens)
        run_model(output_train,problem1_id)
        pd.append(mean(pd_values))

    plt.figure(2)
    ax = plt.axes()
    ax.plot(x,pd, linestyle= 'dashdot', marker='+', color='blue')
    ax.margins(x=0,y=0)
    ax.grid(False)   
    ax.set_xlabel('Number of SUs')
    ax.set_ylabel('(Pd)')
    plt.grid(zorder=0,linestyle='dotted') 
    plt.savefig('PdVSSUs metric .pdf')
    plt.close()
        
        

def plot_Pe_Vs_Putx_sigma(problem1_id):
    """
    this function plot the error probability for different configurations
    of the primary user signal power and noise variance

    Parameters
    ----------
    problem1_id : str
        DNN model id.

    Returns
    -------
    None.

    """
    global p_t_dB 
    global p_t 
    global sigma_v
    sigma = [15,10,5]
    p_t_dB = [10,15,20,25,30,35,40]
    p_t= [10*(x/10) for x in p_t_dB]
    plots = dict()
    for sig in sigma:
        sigma_v = sig
        for tx in p_t_dB:
            file_id = str(num_sens)
            print(num_sens)
            if os.path.isfile('trainData'+file_id+'.data'):
                with open('trainData'+str(num_sens)+'.data', 'rb') as filehandle:
                    output_train = pickle.load(filehandle)
            else:
                output_train = generate_data('2',num_sens)
            run_model(output_train,problem1_id)
            plots[sig].append(mean(Pe_values))
    barWidth = 0.5 
    plt.figure(2)
    ax = plt.axes()
    ax.plot(p_t_dB,plots.get(sigma[0]), linestyle= 'dashdot', marker='+', color='red',label='$\sigma=15dB$')
    ax.plot(p_t_dB,plots.get(sigma[1]), linestyle= 'dashdot', marker='+', color='blue',label='$\sigma=10dB$')
    ax.plot(p_t_dB,plots.get(sigma[2]), linestyle= 'dashdot', marker='+', color='green',label='$\sigma=5dB$')
    ax.margins(x=0,y=0)
    ax.grid(False)
    plt.legend()
    ax.set_xlabel('Transmit Power')
    ax.set_ylabel('average Pe') 
    plt.savefig('PeVSPUp metric .pdf')
    plt.close()            
        
        
        

def plot_Pe_Vs_SU(problem1_id):
    """
    this function plot the error probability for different
    number of secondary users configurations

    Parameters
    ----------
    problem1_id : str
        DNN model id.

    Returns
    -------
    None.

    """
    global num_sens
    pe = dict()
    x = [10,20,30]
    for su in x:
        num_sens = su
        file_id = str(num_sens)
        if os.path.isfile('trainData'+file_id+'.data'):
            with open('trainData'+file_id+'.data', 'rb') as filehandle:
                output_train = pickle.load(filehandle)
        else:
            output_train = generate_data('2',num_sens)
        run_model(output_train,problem1_id)
        #pe[num_sens].append(mean(Pe_values))

    plt.figure(2)
    ax = plt.axes()
    ax.plot(pe.get(num_sens[0]) , linestyle= 'dashdot', marker='+', color='red',label='N=10')
    ax.plot(pe.get(num_sens[1]) , linestyle= 'dashdot', marker='+', color='blue',label='N=20')
    ax.plot(pe.get(num_sens[2]) , linestyle= 'dashdot', marker='+', color='black',label='N=30')
    ax.margins(x=0,y=0)
    ax.grid(False)
    plt.legend()
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Pe')
    plt.show()  
    plt.savefig('Pe metric .pdf')
    plt.close()
    
def correlation_vs_pe(problem1_id):
    """
    this function plots the pe variantion depending on the correlation coef

    Parameters
    ----------
    problem1_id : str
       the dnn model id 

    Returns
    -------
    None.

    """
    
    global d_ref
    avg_pe = []
    ref= [5,10,15,20,25,30,35,40,45,50]
    
    for x in ref:
        d_ref = x
        if os.path.isfile('trainData'+str(num_sens)+'.data'):
            with open('trainData'+str(num_sens)+'.data', 'rb') as filehandle:
                output_train = pickle.load(filehandle)
        else:
            output_train = generate_data('2',num_sens)
        run_model(output_train,problem1_id)
        avg_pe.append(mean(Pe_values))
    plt.figure(2)
    ax = plt.axes()
    ax.plot(ref,avg_pe, linestyle= 'dashdot', marker='+', color='red')
    ax.margins(x=0,y=0)
    ax.grid(False)    
    ax.set_xlabel('Correlation Reference Distance')
    ax.set_ylabel('The average (Pe)')
    plt.grid(zorder=0,linestyle='dotted')
    #plt.show()  
    plt.savefig('PeVSdRef metric .pdf')
    plt.close()
            
    
    
    
    
    
    
    

def Time_complexity_Vs_SU_DNN(problem1_id):
    global num_sens
    num_sens = [10,20,30,40,50]
    time_complexity = []
    for su in num_sens:
        num_sens = su
        file_id = str(num_sens)
        if os.path.isfile('trainData'+file_id+'.data'):
            with open('trainData'+file_id+'.data', 'rb') as filehandle:
                output_train = pickle.load(filehandle)
        else:
            output_train = generate_data('2',num_sens)

        snrs_train = scaling_data(output_train)
        signal_gain= np.array(output_train['gain'])
        X_train, X_val, y_train, y_val = train_test_split(snrs_train, snrs_train, test_size=0.30)
        model = BivariateDNNModel.choose_model("dropout")
        history = BivariateDNNModel.train_model(model, X_train, X_val, y_train, y_val, BivariateDNNModel.bivariate_loss_gain(gain=signal_gain)) 
        BivariateDNNModel.eval_metric(model, history, "loss", problem1_id)
        test_data = scaling_data(output_train)
        start = time.time()
        model.predict(test_data)
        time_complexity.append(time.time() - start)
        
    return time_complexity

def Time_complexity_Vs_SU_Numerical():
    global num_sens
    num_sens = [10,20,30,40,50]
    time_complexity = []
    for su in num_sens:
        num_sens = su
        file_id = str(num_sens)
        if os.path.isfile('trainData'+file_id+'.data'):
            with open('trainData'+file_id+'.data', 'rb') as filehandle:
                output_train = pickle.load(filehandle)
        else:
            output_train = generate_data('2',num_sens) 
        snrs_test = scaling_data(output_train)
        start = time.time()
        spectrum_sensing = SpectrumSensing()
        spectrum_sensing.generate_thresholds(snrs_test)
        time_complexity.append(time.time() - start)
    
    return time_complexity

def plot_Time_complexity(problem1_id):
    """
    this function plot the time complexity comparision 
    between the DNN and the numerical model using 
    different number of SU configurations

    Parameters
    ----------
    problem1_id : str
        The model id.

    Returns
    -------
    None.

    """
    
    # set width of bar
    barWidth = 0.25
    fig = plt.subplots(figsize =(12, 8))
     
    dnn = Time_complexity_Vs_SU_DNN(problem1_id)
    numerical = Time_complexity_Vs_SU_Numerical()
    
    
     
    # Set position of bar on X axis
    br1 = np.arange(len(numerical))
    br2 = [x + barWidth for x in br1]
    br3 = [x + barWidth for x in br2]
     
    # Make the plot
    plt.bar(br1, numerical, color ='green', width = barWidth,
            edgecolor ='grey', label ='Numerical')
    plt.bar(br2, dnn, color ='blue', width = barWidth,
            edgecolor ='grey', label ='DNN')
    
     
    # Adding Xticks
    plt.xlabel('number of SU', fontweight ='bold', fontsize = 12)
    plt.ylabel('Time (ms)', fontweight ='bold', fontsize = 12)
    plt.xticks([r + barWidth for r in range(len(numerical))],
            ['N=10', 'N=20', 'N=30', 'N=40', 'N=50'])
    plt.grid(zorder=0,linestyle='dotted')    
    plt.legend()
    #plt.show()
    plt.savefig('histogram_P1.pdf')    

    
    

# def plot_menu(plot,problem1_id):
#     """
#     This is the menu function for the plotting choice

#     Parameters
#     ----------
#     plot : str
#         '1':for minimum Pd Vs number of SU.
#         '2':for Pe VS Pu transmition power and noise variance.
#         '3':for Pe Vs number of SU
#         '4':for Time comlexity DNN vs Numerical
#         '5': for the correletion vs pe
#     problem1_id : str
#         DNN model id.

#     Returns
#     -------
#     None.

#     """
    


#     if plot == '1':
#         plot_mininumPd_Vs_SU(problem1_id)
#     if plot == '2':
#         plot_Pe_Vs_Putx_sigma(problem1_id)
#     if plot == '3':
#         plot_Pe_Vs_SU(problem1_id)
#     if plot == '4':
#         plot_Time_complexity(problem1_id)
#     if plot== '5':
#         correlation_vs_pe(problem1_id)
        
            
            
            
            
    
    
         

def main():
    
    
    problem1_id = 'bivariate_problem'
    # print("Please define  your plot choice\n")
    # print("press 1 for minimum Pd Vs number of SU\n")
    # print("press 2 for Pe VS Pu transmition power and noise variance\n")
    # print("press 3 for Pe Vs number of SU\n")
    # print("press 4 for Time comlexity DNN vs Numerical\n")
    # print("press 5 for Pe VS correlation coef\n")
    
    # plot = input("press your choice: ") 
    # plot_menu(plot,problem1_id)

    plot_Pe_Vs_SU(problem1_id)
    plot_Pe_Vs_Putx_sigma(problem1_id)
    plot_averagePe_Vs_SU(problem1_id)    
    correlation_vs_pe(problem1_id)
    plot_Time_complexity(problem1_id)
    
    
    
        



    
    
    
  
if __name__ == '__main__': 
    main()

